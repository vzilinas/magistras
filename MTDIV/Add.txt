
\subsection{Skatinamasis mašininis mokymasis}
Skatinamasis mokymasis yra algoritmais priklausantys mašininio mokymosi algoritmų grupei. Mašininis mokymasi algoritmai - algoritmai, kurie kuria matematinius modelius paremtus mokymosi duomenimis ir darinčius sprendimus, kurie nėra suprogramuoti \cite{bishoppattern}. Mašininis mokymasis neseniai išpopuliarėjo dėko poreiko dirbti su dideliais kiekiais nerušiuotų ir nestrukturizuotų duomenų ir dėl namų kopiuterių galios padidėjimo iki lygio, kuris leidžia spręsti mašininio mokymosi skaičiavimus \cite{whypopular}.   

Standartiniame skatinamojo mašininio mokymosi algoritme agentas (angl. agent) yra prijungtas prie aplinkos (angl. environment) per stebėjimus(angl. perception) ir veiksmus(angl. action). Kiekvieną žingsnį agentas atlieką veiksmą ir to veiksmo vertė yra perduodama agentui per skatinamąjį signalą. Agentas turi rinktis veiksmus, kurie per ilgą laiko tarpą didins veiksmo įverčius. Tai pasiekiamą per tam tikrą laiko tarpą systematiškai atliekant bandymus ir klystant, su papildoma algoritmu pagalba siekiant papdidinti efektyvumą ir sprendimų stabilumą \cite{reinf}.     

\subsection{Deep Q Network algoritmas}
Deep Q Network yra Q Learning implementacija naudojant giliuosius neuroninius tinklus. Q Learning - skatinamojo mokymosi algortimas, kuris bet kokiam baigtiniam Markovo pasirinkimo procesui randa optimalius sprendimus maximizuojančius galutinio rezultato gavimą per bet kokį kiekį žingsnių pradedant nuo esamos būsenos \cite{melo2001convergence}. 
Q Learning naudoja Q funkciją, kurios įeigą yra būsenos ir veiksmo kombinacija, o rezultatas yra atlygio apromiksimacija. Pradžioje visos Q reikšmės yra 0 ir algoritmas atlikdamas veiksmus pildo lentelę atnaujintaumis reikšmėmis. Tačiau, kai veiksmų ir būsenų pasidaro per daug Q Learning algortimo nebeužtenka ir tenka naudoti giliuosius neuroninius tinklus.  

Deep Q Network skyriasi nuo Q Learning tuo, kad vietoj būsenos ir veiksmų į jį paduodama būseną ir jis gražina Q reikšmę visų įmanomų veiksmų. Taip pat Deep Q Network naudoja patirties pakartojimą (angl. expirience replay) - vietoj to, kad kurti sprendimą pagal paskutinį veiksmą yra paduodamas rinkinys atsitiktinių veiksmų pagal kurį algoritmas gali efektyviau mokytis. 

Deep Q Network algoritmo veikimas\cite{handson}:
\begin{enumerate}
    \item Perduoti aplinkos būseną į Deep Q Network, kuris gražins visus įmanomus veiksmus būsenai.
    \item Pasirinkti veiksmą naudojant \(\epsilon\)-greedy strategiją, kuriuo metu pasirenkamas atsitiktinis veiksmas arba pasirenkamas veiksmas turintis didžiausia Q reikšmę.
    \item Įvykdomas veiksmas ir pereinama į naują būseną. Šis perėjimas išsaugomas kaip patirties pakartojimo kortežas susidarantis iš būsenos, veiksmo, atlygio ir naujos būsenos.
    \item Pasirenkamas atsitiktinis rinkints perėjimų iš patirties paraktojimo kortežų rinkinio ir apskaičiuojamas nuostolis (angl. loss). \[Loss=(r + \gamma max_{a'} Q(s',a';\theta') - Q(s,a;\theta))^2\]
    \item Atlikti gradiento nusileidimą su tikrais tinklo parametrais siekiant sumažinti nuostolį.
    \item Po kiekvienos iteracijos, perkelti tikrojo tinklo svorius į pasirinkto tinklo svorius.
    \item Visi žingsniai kartojami iki nustatytos pabaigos.
\end{enumerate}
\subsection{Soft Actor Critic algortimas}
Soft Actor Critic skiriasi nuo kitų skatinamojo mokymosi algoritmų tuo kad jis bando ne tik pasiekti didžiausią bendrą atlygį, tačiau ir didinti strategijos entropiją. Entropijos didinimas reiškia, kad Soft Actor Critic algoritmas yra labiau linkęs į tyrinėjimą (angl. exploration) nei kiti algoritmai, ko pasekoje užtikrinama, kad algoritmas nesirinks visada tokio pačio veiksmo. 

\subsection{Standartinė konfiguracija}
Sukurto sprendimo rezultatai bus lyginami su standartine konfiguracija tik su pakeistu skaičiavimo komponentų lygiagretumu atsižvelgiant į įrangos skaičiavimo gijų kiekį. Šiuo rezultatu bus tikrinama, ar Apache Heron pats negali taip pat arba geriau susitvarkyti su įeinančiu duomenų kiekiu nedidinant vėlinimo. Standartinės konfiguracijos naudojami balansavimo sprendimai \cite{twitterHeron}:
\begin{itemize}
    \item Srautinės apdorojimo sistemos priešslėgis (angl. backpressure) – kai pastebimas jog tam tikras skaičiavimo kompoenentas nespėja susidoroti su ateinančiu duomenu kiekiu ir praneša prieš tai einantiems skaičiavimo komponentams lėtinti duomenų patekimo greiiti. Heron yra implementuotas šaltinio priešslėgis, kuris veikia kaip buferis ant kiekvienos jungties tarp komonentų ir kai buferis prisipildo iki tam tikro taško įjungiamas priešslėgio režimas, kuris sulėtina srautinio apdorojimo sistemos greiti iki lėčiausio skaičiavimo komponento greičio. 
    \item Šiukšių surinktuvo (angl. garbage collector) optimizacija - kai srautinio apdorojimo sistema gaudavo dideli kiekį duomenų vienu metu ir užsipildydavo visa turima sistemos atmintis, kiekvieną karta gaunant nauja įraša buvo paleidžiamas šiukšlių surinktuvas, kuris naudoja daug resursų. Kad išspręsti šią problemą Heron periodiškai tikrina srautinio apdorojimo sistemos atminties talpą ir patenkančių duomenų dydį ir jeigu duomenų dydis pasidaro didesnis nei talpa tai duomenų paėmimo greitis sumažinamas per pusę ir taip kartojama kol pasiekiamas stabilus greitis. 
    \item Resursų rezervacija - kiekvienas konteineris (komponentų rinkinys) turi jam priskrita resursų kiekį ir prieš palaiedžiant srautinio apdorojimo sistema Heron rezervuoja reikiamus resursus ir jeigu komponentai bando pasiekti daugiau resursu negu jiems rezervuota yra lėtinama sistema taip užtikrinant stabilumą \cite{fu2015streaming}.  
\end{itemize}
\subsection{Balansavimas naudojant REINFORCE algoritmą}
Sukurtas sprendimas naudojantis Deep Q network ir Soft Actor Critic taip pat bus lyginamas su \cite{vaquero2018autotuning} aprašytų REINFORCE algortimu. Staripsnis nagrinėja automatinį balansavimą srautinio apdorojimo sistemų Apache Spark platformoje. Straipsnyje nagrinėjamas sprendimas susidaro iš trijų sistemų:
\begin{enumerate}
    \item Sistema, kurioje uš anksto sugeneruoti konfiguracijų deriniai leidžiami srautinio apdorojimo sistemose ir surenkamos metrikos bei konfiguracijos įverčiai. Gauti duomenis analyzuojami naudojant Factor Analysis + k-means ir gaunamas sąrašas pagrindinių metrikų bei konfiguracijos elementai darantys daugiausiai įtakos greitaveikai. Ši sistema naudojama vieną kartą prieš leidžiant sekančią sistemą. 
    \item Sistema, kurioje surinktos metrikos ir konfiguracijos elementai yra leidžiami iš naujo ir naudojant Lasso path analizę konfiguracijos elementų sąrašas surušiuojamas pagal įtaką greitaveikai. Tai daroma siekiant statistiškai užtikrinti, jog pasirinkti konfiguracijos elementai yra įtakingiausi. Ši sistema naudojama vieną kartą prieš leidžiant sekančią sistemą.
    \item Pagrindinė mašininio mokymosi sistema, naudojanti REINFORCE algoritmą, kuri naudodamas surikiuotų konfiguracijos elementų sąrašą ir pagrindinių metrikų sąraša periodiškai atnaujina konfiguraciją. Ši sistema paleidžiama tuo pačiu metu kaip ir srautinio apdorojimo sistema ir veikia visą laiką, kol paleistas ekspermientas.
\end{enumerate}  
Eksperimentinis sprendimas buvo sukonfiguruotas kas 5 minutes atnaujinti vieną konfiguracijos elementą. Autoriams pavyko pasiekti 70\% sumažinta vėlinimmą po 50 minučių ir mokymas pilnai konvergavo po 11 valandų. Magistro darbe bus naudojamas supaprastintas eksperimentas: bus implementuojamas REINFORCE algoritmas ir lyginamas su Deep Q Network ir Soft Actor Critic algoritmais.